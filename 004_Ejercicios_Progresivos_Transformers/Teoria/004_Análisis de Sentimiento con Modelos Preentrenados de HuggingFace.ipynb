{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Análisis de Sentimiento con Modelos Preentrenados de HuggingFace\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "En esta actividad vamos a utilizar un modelo de estado del arte ya entrenado para analizar el sentimiento de frases en español con apenas unas líneas de código, gracias a la librería HuggingFace Transformers.\n",
    "\n",
    "Vamos a mostrar cómo es posible aprovechar el poder de los Transformers como BERT sin necesidad de entrenar redes neuronales desde cero.\n",
    "\n",
    "### ¿Por qué esto es revolucionario?\n",
    "\n",
    "Hasta ahora entrenamos modelos desde cero:\n",
    "- Perceptrón con 6 ejemplos\n",
    "- MLP con 10 ejemplos\n",
    "- LSTM con 10 ejemplos\n",
    "\n",
    "**El problema**: Con tan pocos datos, los modelos aprenden poco y generalizan mal.\n",
    "\n",
    "**La solución**: Usar modelos que ya fueron entrenados con millones de textos. Esto se llama **transfer learning** (aprendizaje por transferencia)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Instalación de la librería transformers\n",
    "\n",
    "Si estás en Google Colab, instalá la librería con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. ¿Qué es BETO?\n",
    "\n",
    "Antes de usar el modelo, entendamos qué estamos cargando.\n",
    "\n",
    "### BETO = BERT en Español\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) es una arquitectura de modelo de lenguaje desarrollada por Google en 2018 que revolucionó el NLP.\n",
    "\n",
    "**BETO** es la versión en español de BERT, entrenada específicamente con textos en español.\n",
    "\n",
    "### Características de BETO:\n",
    "\n",
    "- **Entrenado con**: Wikipedia en español (millones de artículos)\n",
    "- **Arquitectura**: Transformer con atención bidireccional\n",
    "- **Parámetros**: ~110 millones\n",
    "- **Tiempo de entrenamiento**: Días en múltiples GPUs\n",
    "- **Costo de entrenamiento**: Miles de dólares en recursos computacionales\n",
    "\n",
    "### ¿Qué significa \"preentrenado\"?\n",
    "\n",
    "BETO ya aprendió:\n",
    "- Gramática del español\n",
    "- Vocabulario extenso\n",
    "- Relaciones semánticas entre palabras\n",
    "- Contextos en los que aparecen las palabras\n",
    "\n",
    "Nosotros vamos a usar ese conocimiento para análisis de sentimiento, sin tener que entrenar nada desde cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3. Cargando el modelo de análisis de sentimiento\n",
    "\n",
    "Vamos a usar `finiteautomata/beto-sentiment-analysis`, que es BETO ya ajustado específicamente para clasificar sentimientos en español.\n",
    "\n",
    "### ¿Qué es un pipeline?\n",
    "\n",
    "Un **pipeline** en HuggingFace encapsula todo el proceso:\n",
    "1. Tokenización del texto\n",
    "2. Conversión a formato del modelo\n",
    "3. Inferencia (predicción)\n",
    "4. Post-procesamiento de resultados\n",
    "\n",
    "Todo esto en una sola línea de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "print(\"Cargando modelo BETO para análisis de sentimiento...\")\n",
    "print(\"Este proceso descarga el modelo (puede tardar unos segundos la primera vez)\\n\")\n",
    "\n",
    "# Cargamos el pipeline de análisis de sentimientos con el modelo en español\n",
    "clasificador = pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n",
    "\n",
    "print(\"Modelo cargado exitosamente\")\n",
    "print(f\"Modelo: {clasificador.model.config._name_or_path}\")\n",
    "print(f\"\\nEste modelo fue ajustado (fine-tuned) específicamente para español.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4. Comparación con nuestros modelos anteriores\n",
    "\n",
    "Antes de probar BETO, recordemos las limitaciones de nuestros modelos:\n",
    "\n",
    "| Aspecto | Nuestros modelos | BETO |\n",
    "|---------|-----------------|------|\n",
    "| Datos de entrenamiento | 6-10 frases | Millones de textos |\n",
    "| Vocabulario | 10-50 palabras | ~30,000 tokens |\n",
    "| Parámetros | Cientos/Miles | 110 millones |\n",
    "| Tiempo de entrenamiento | Segundos | Días en GPUs |\n",
    "| Comprensión del lenguaje | Básica | Avanzada |\n",
    "\n",
    "Esta diferencia de escala es lo que hace que BETO sea tan poderoso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 5. Evaluación de frases variadas\n",
    "\n",
    "Ahora vamos a probar el modelo con frases reales, incluyendo expresiones típicas de Argentina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\n",
    "    \"Este lugar es espectacular, lo recomiendo totalmente\",\n",
    "    \"Una decepción total. No pienso volver\",\n",
    "    \"Más o menos... esperaba otra cosa\",\n",
    "    \"Qué buena onda la atención, me encantó\",\n",
    "    \"Mala calidad, pésimo servicio\",\n",
    "    \"Zafa, pero nada especial\",\n",
    "    \"Me sentí muy bien atendido\",\n",
    "    \"Una estafa. Me arrepiento totalmente\",\n",
    "    \"Todo excelente, 10 puntos\",\n",
    "    \"Nunca más. Fue un desastre\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREDICCIONES DE BETO EN FRASES VARIADAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clasificamos cada frase\n",
    "resultados = clasificador(frases)\n",
    "\n",
    "# Mostramos los resultados\n",
    "for i, (frase, resultado) in enumerate(zip(frases, resultados), 1):\n",
    "    sentimiento = resultado['label']\n",
    "    confianza = resultado['score']\n",
    "    \n",
    "    print(f\"\\nFrase {i}: '{frase}'\")\n",
    "    print(f\"  Predicción: {sentimiento} (confianza: {confianza:.2f})\")\n",
    "    \n",
    "    # Indicador de confianza\n",
    "    if confianza >= 0.9:\n",
    "        print(f\"  Confianza: Muy alta\")\n",
    "    elif confianza >= 0.7:\n",
    "        print(f\"  Confianza: Alta\")\n",
    "    elif confianza >= 0.5:\n",
    "        print(f\"  Confianza: Media\")\n",
    "    else:\n",
    "        print(f\"  Confianza: Baja (poco seguro)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 6. Análisis de expresiones rioplatenses\n",
    "\n",
    "Vamos a probar específicamente con expresiones típicas de Argentina para ver si BETO las entiende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "expresiones_argentinas = [\n",
    "    \"Está re copado este lugar\",\n",
    "    \"Qué garrón, me clavaron\",\n",
    "    \"La pasé bomba, volvería sin dudarlo\",\n",
    "    \"Un bodrio total, no lo banco más\",\n",
    "    \"Me re sirvió, gracias\",\n",
    "    \"Qué embole, fue un bajón\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BETO CON EXPRESIONES RIOPLATENSES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "resultados = clasificador(expresiones_argentinas)\n",
    "\n",
    "for frase, resultado in zip(expresiones_argentinas, resultados):\n",
    "    print(f\"\\nFrase: '{frase}'\")\n",
    "    print(f\"  {resultado['label']} ({resultado['score']:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Observá cómo BETO entiende modismos argentinos como:\")\n",
    "print(\"  'copado', 'garrón', 'bomba', 'bodrio', 'embole', 'bajón'\")\n",
    "print(\"Esto es gracias a su entrenamiento con textos en español variados.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 7. Casos desafiantes: Opiniones mixtas y matices\n",
    "\n",
    "Probemos con casos más complejos que suelen confundir a modelos simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_complejos = [\n",
    "    \"La comida estuvo bien pero el servicio fue horrible\",\n",
    "    \"No estuvo mal, aunque esperaba más por el precio\",\n",
    "    \"Pensé que iba a ser un desastre pero me sorprendió para bien\",\n",
    "    \"Excelente producto, lástima la demora en el envío\",\n",
    "    \"No puedo decir que no me gustó, porque sería mentir\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CASOS COMPLEJOS: OPINIONES MIXTAS Y MATICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "resultados = clasificador(casos_complejos)\n",
    "\n",
    "for i, (frase, resultado) in enumerate(zip(casos_complejos, resultados), 1):\n",
    "    print(f\"\\nCaso {i}: '{frase}'\")\n",
    "    print(f\"  Predicción: {resultado['label']} (confianza: {resultado['score']:.2f})\")\n",
    "    \n",
    "    # Análisis del caso\n",
    "    if i == 1:\n",
    "        print(f\"  Análisis: Opinión mixta (comida bien, servicio mal)\")\n",
    "    elif i == 2:\n",
    "        print(f\"  Análisis: Sentimiento tibio con matices\")\n",
    "    elif i == 3:\n",
    "        print(f\"  Análisis: Giro de expectativa negativa a positiva\")\n",
    "    elif i == 4:\n",
    "        print(f\"  Análisis: Positivo con salvedad\")\n",
    "    elif i == 5:\n",
    "        print(f\"  Análisis: Doble negación (no... no = sí)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 8. Comparación directa: ¿Cómo se compara con LSTM?\n",
    "\n",
    "Recordemos que con LSTM entrenamos con 10 frases. BETO, en cambio, fue entrenado con millones de textos.\n",
    "\n",
    "### Ventajas observadas de BETO:\n",
    "\n",
    "1. **Vocabulario extenso**: Entiende palabras que nunca vio en nuestro pequeño dataset\n",
    "2. **Contexto bidireccional**: Mira toda la frase simultáneamente, no solo de izquierda a derecha\n",
    "3. **Modismos y expresiones**: Reconoce jerga argentina sin entrenamiento específico\n",
    "4. **Matices**: Maneja mejor opiniones mixtas y giros en el sentimiento\n",
    "5. **Confianza calibrada**: Los scores reflejan mejor la certeza del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 9. Probá tus propias frases\n",
    "\n",
    "Ahora te toca experimentar. Agregá frases propias y observá cómo se comporta BETO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregá tus propias frases acá\n",
    "mis_frases = [\n",
    "    \"Tu frase 1\",\n",
    "    \"Tu frase 2\",\n",
    "    \"Tu frase 3\"\n",
    "]\n",
    "\n",
    "# Descomentá estas líneas cuando tengas tus frases\n",
    "# resultados_propios = clasificador(mis_frases)\n",
    "# for frase, resultado in zip(mis_frases, resultados_propios):\n",
    "#     print(f\"\\nFrase: '{frase}'\")\n",
    "#     print(f\"  Predicción: {resultado['label']} ({resultado['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 10. Reflexión final\n",
    "\n",
    "### ¿Qué aprendimos?\n",
    "\n",
    "1. **Transfer learning es poderoso**: No necesitamos entrenar desde cero para obtener buenos resultados en NLP.\n",
    "\n",
    "2. **Modelos preentrenados en español**: Existen modelos como BETO que entienden texto coloquial, incluso modismos regionales.\n",
    "\n",
    "3. **Facilidad de uso**: Con HuggingFace Transformers podemos usar modelos de estado del arte con apenas unas líneas de código.\n",
    "\n",
    "4. **Escala importa**: Un modelo entrenado con millones de ejemplos es cualitativamente diferente a uno entrenado con 10.\n",
    "\n",
    "### Comparación con nuestros modelos anteriores:\n",
    "\n",
    "**Perceptrón (Lab 1)**:\n",
    "- Entrenamiento: 6 frases, segundos\n",
    "- Limitación: Modelo lineal, bag-of-words\n",
    "- Resultado: Clasificación básica\n",
    "\n",
    "**MLP (Lab 2)**:\n",
    "- Entrenamiento: 10 frases, segundos\n",
    "- Mejora: Patrones no lineales\n",
    "- Limitación: Sigue sin orden, bag-of-words\n",
    "\n",
    "**LSTM (Lab 3)**:\n",
    "- Entrenamiento: 10 frases, minutos\n",
    "- Mejora: Procesa secuencias, embeddings\n",
    "- Limitación: Dataset pequeño, vocabulario limitado\n",
    "\n",
    "**BETO (Lab 4)**:\n",
    "- Entrenamiento: Millones de textos, días en GPUs\n",
    "- Mejora: Comprensión profunda del español\n",
    "- Uso: Sin entrenar, solo inferencia\n",
    "\n",
    "### ¿Por qué esto cambió el NLP?\n",
    "\n",
    "Antes de 2018 (pre-BERT), cada tarea requería:\n",
    "1. Conseguir dataset grande específico\n",
    "2. Entrenar modelo desde cero\n",
    "3. Esperar que generalizara\n",
    "\n",
    "Después de 2018 (era Transformer):\n",
    "1. Usar modelo preentrenado\n",
    "2. Opcionalmente: ajustar con pocos ejemplos (fine-tuning)\n",
    "3. Obtener resultados superiores\n",
    "\n",
    "### Conceptos clave para entender LLMs:\n",
    "\n",
    "Lo que vimos hoy es la base de cómo funcionan los LLMs modernos:\n",
    "\n",
    "- **Preentrenamiento masivo**: GPT, BERT, LLaMA se entrenan con billones de palabras\n",
    "- **Arquitectura Transformer**: Atención en lugar de procesamiento secuencial\n",
    "- **Transfer learning**: El conocimiento se transfiere entre tareas\n",
    "- **Emergencia de capacidades**: Con suficiente escala, surgen habilidades no programadas explícitamente\n",
    "\n",
    "### ¿Qué sigue?\n",
    "\n",
    "En las próximas clases vamos a profundizar en:\n",
    "- Arquitectura Transformer en detalle\n",
    "- Mecanismo de atención\n",
    "- Cómo se entrenan modelos como GPT\n",
    "- Fine-tuning y prompting\n",
    "- Construcción de aplicaciones con LLMs\n",
    "\n",
    "Hoy vimos **qué** pueden hacer estos modelos. Próximamente veremos **cómo** funcionan por dentro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
