{"cells":[{"cell_type":"markdown","id":"cell-0","metadata":{"id":"cell-0"},"source":["# Perceptrón para Análisis de Sentimiento en Español\n","\n","## Objetivo\n","\n","En esta actividad vas a implementar desde cero un modelo de Perceptrón en Python usando solo NumPy. Vamos a entrenarlo con frases breves escritas en español rioplatense para que aprenda a reconocer si una frase tiene un sentimiento positivo o negativo.\n","\n","El objetivo es entender cómo funciona una neurona artificial básica, cómo se ajustan sus pesos durante el aprendizaje, y cómo puede hacer predicciones en base a un conjunto pequeño de frases.\n","\n","### ¿Qué es un perceptrón?\n","\n","El perceptrón es el modelo más simple de neurona artificial. Fue propuesto en 1958 por Frank Rosenblatt y representa la base fundamental de las redes neuronales modernas.\n","\n","**Funcionamiento básico:**\n","1. Recibe múltiples entradas (features)\n","2. Multiplica cada entrada por un peso\n","3. Suma todos los resultados y agrega un sesgo (bias)\n","4. Aplica una función de activación para decidir la salida\n","\n","Es un modelo lineal que solo puede aprender patrones que sean linealmente separables."]},{"cell_type":"markdown","id":"cell-1","metadata":{"id":"cell-1"},"source":["## 1. Datos de entrenamiento\n","\n","Vamos a usar un pequeño conjunto de frases etiquetadas como positivas (1) o negativas (0). Las frases son simples como las que podríamos encontrar en una conversación cotidiana en Argentina.\n","\n","También definimos un vocabulario básico de palabras clave que aparecen con frecuencia y que nos pueden ayudar a inferir el sentimiento."]},{"cell_type":"code","execution_count":1,"id":"cell-2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-2","executionInfo":{"status":"ok","timestamp":1760482613716,"user_tz":180,"elapsed":21,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"a877c305-1193-494b-d52f-78453e05f500"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total de frases: 6\n","Vocabulario: 10 palabras clave\n","\n","Primera frase: 'Amo el verano en Buenos Aires' → Sentimiento: Positivo\n"]}],"source":["import numpy as np\n","\n","# Frases con su etiqueta de sentimiento (1 = positivo, 0 = negativo)\n","frases = [\n","    \"Amo el verano en Buenos Aires\",\n","    \"No me gusta el tráfico matutino\",\n","    \"Este asado está espectacular\",\n","    \"Qué bajón, perdí el colectivo\",\n","    \"Me encanta salir los domingos\",\n","    \"Detesto el calor húmedo\"\n","]\n","\n","etiquetas = np.array([1, 0, 1, 0, 1, 0])  # Etiquetas correspondientes\n","\n","# Vocabulario manual con palabras claves de carga emocional\n","vocabulario = [\"amo\", \"no\", \"gusta\", \"asado\", \"espectacular\", \"bajón\", \"perdí\", \"detesto\", \"calor\", \"húmedo\"]\n","\n","print(f\"Total de frases: {len(frases)}\")\n","print(f\"Vocabulario: {len(vocabulario)} palabras clave\")\n","print(f\"\\nPrimera frase: '{frases[0]}' → Sentimiento: {'Positivo' if etiquetas[0] == 1 else 'Negativo'}\")"]},{"cell_type":"markdown","id":"cell-3","metadata":{"id":"cell-3"},"source":["## 2. Representación numérica: Vectorización de frases\n","\n","Las redes neuronales no pueden procesar texto directamente. Necesitamos convertir cada frase en un vector numérico.\n","\n","### Bag of Words (Bolsa de Palabras)\n","\n","Vamos a usar una representación simple llamada **bag of words**: para cada frase, creamos un vector binario que indica si cada palabra del vocabulario aparece (1) o no (0) en la frase.\n","\n","**Ejemplo:**\n","- Vocabulario: [\"amo\", \"no\", \"gusta\", \"asado\"]\n","- Frase: \"Amo el asado\"\n","- Vector: [1, 0, 0, 1] (tiene \"amo\" y \"asado\", no tiene \"no\" ni \"gusta\")\n","\n","**Limitación importante:** Este método no captura el orden de las palabras ni el contexto. \"No me gusta\" y \"me gusta\" tendrían vectores muy similares."]},{"cell_type":"code","execution_count":2,"id":"cell-4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-4","executionInfo":{"status":"ok","timestamp":1760482740382,"user_tz":180,"elapsed":13,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"2f2826ae-1648-4e68-adac-1e34f089a90c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz de features (X):\n","Forma: (6, 10) (6 frases × 10 features)\n","\n","Vectores generados:\n","Frase 1: [1 0 0 0 0 0 0 0 0 0] → Sentimiento: Positivo\n","Frase 2: [0 1 1 0 0 0 0 0 0 0] → Sentimiento: Negativo\n","Frase 3: [0 0 0 1 1 0 0 0 0 0] → Sentimiento: Positivo\n","Frase 4: [0 0 0 0 0 0 1 0 0 0] → Sentimiento: Negativo\n","Frase 5: [0 0 0 0 0 0 0 0 0 0] → Sentimiento: Positivo\n","Frase 6: [0 0 0 0 0 0 0 1 1 1] → Sentimiento: Negativo\n"]}],"source":["def vectorizar(frase, vocabulario):\n","    \"\"\"\n","    Convierte una frase en un vector binario según el vocabulario.\n","\n","    Args:\n","        frase: String con la frase a vectorizar\n","        vocabulario: Lista de palabras clave\n","\n","    Returns:\n","        Array de numpy con 1s y 0s\n","    \"\"\"\n","    tokens = frase.lower().split()\n","    return np.array([1 if palabra in tokens else 0 for palabra in vocabulario])\n","\n","# Aplicamos la función a todas las frases\n","X = np.array([vectorizar(frase, vocabulario) for frase in frases])\n","\n","print(\"Matriz de features (X):\")\n","print(f\"Forma: {X.shape} (6 frases × 10 features)\\n\")\n","print(\"Vectores generados:\")\n","for i, (frase, vector) in enumerate(zip(frases, X)):\n","    print(f\"Frase {i+1}: {vector} → Sentimiento: {'Positivo' if etiquetas[i] == 1 else 'Negativo'}\")"]},{"cell_type":"markdown","id":"cell-5","metadata":{"id":"cell-5"},"source":["## 3. Definición del modelo: el Perceptrón\n","\n","Un perceptrón es una función matemática que:\n","\n","1. **Multiplica** cada entrada por un peso (weight)\n","2. **Suma** los resultados y agrega un sesgo (bias)\n","3. **Aplica** una función de activación para decidir si \"dispara\" o no\n","\n","### Fórmula matemática:\n","\n","```\n","z = (x₁ × w₁) + (x₂ × w₂) + ... + (xₙ × wₙ) + bias\n","salida = función_activación(z)\n","```\n","\n","### Función de activación escalón:\n","\n","```\n","Si z > 0 → salida = 1 (positivo)\n","Si z ≤ 0 → salida = 0 (negativo)\n","```\n","\n","Vamos a inicializar los pesos aleatoriamente y entrenar el modelo para que aprenda de los errores."]},{"cell_type":"code","execution_count":3,"id":"cell-6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-6","executionInfo":{"status":"ok","timestamp":1760482826315,"user_tz":180,"elapsed":46,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"d61a6bd2-f96a-4f8e-8317-83e89cdd37f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pesos iniciales (aleatorios):\n","  w[amo] = 0.497\n","  w[no] = -0.138\n","  w[gusta] = 0.648\n","  w[asado] = 1.523\n","  w[espectacular] = -0.234\n","  w[bajón] = -0.234\n","  w[perdí] = 1.579\n","  w[detesto] = 0.767\n","  w[calor] = -0.469\n","  w[húmedo] = 0.543\n","\n","Bias inicial: 0.0\n","\n","Funciones definidas: activar() y predecir()\n"]}],"source":["# Inicializamos pesos y bias con valores aleatorios pequeños\n","np.random.seed(42)  # Para reproducibilidad\n","pesos = np.random.randn(len(vocabulario))\n","bias = 0.0\n","\n","print(\"Pesos iniciales (aleatorios):\")\n","for i, (palabra, peso) in enumerate(zip(vocabulario, pesos)):\n","    print(f\"  w[{palabra}] = {peso:.3f}\")\n","print(f\"\\nBias inicial: {bias}\")\n","\n","def activar(suma):\n","    \"\"\"\n","    Función de activación escalón (step function).\n","    Devuelve 1 si la suma es positiva, 0 en caso contrario.\n","    \"\"\"\n","    return 1 if suma > 0 else 0\n","\n","def predecir(x):\n","    \"\"\"\n","    Hace una predicción para un vector de entrada x.\n","\n","    Args:\n","        x: Vector de features (array de numpy)\n","\n","    Returns:\n","        Predicción: 0 o 1\n","    \"\"\"\n","    suma = np.dot(x, pesos) + bias\n","    return activar(suma)\n","\n","print(\"\\nFunciones definidas: activar() y predecir()\")"]},{"cell_type":"markdown","id":"cell-7","metadata":{"id":"cell-7"},"source":["## 4. Entrenamiento del modelo\n","\n","Ahora vamos a entrenar el perceptrón ajustando los pesos según los errores que comete.\n","\n","### Regla de aprendizaje del perceptrón:\n","\n","Para cada ejemplo:\n","1. Calcular la predicción\n","2. Calcular el error: `error = etiqueta_real - predicción`\n","3. Si hay error (≠ 0), ajustar los pesos:\n","   - `peso_nuevo = peso_viejo + (tasa_aprendizaje × error × entrada)`\n","   - `bias_nuevo = bias_viejo + (tasa_aprendizaje × error)`\n","\n","### Parámetros de entrenamiento:\n","\n","- **Tasa de aprendizaje**: Controla qué tan grande es cada ajuste (0.1 es un valor común)\n","- **Épocas**: Cantidad de veces que recorremos todo el dataset\n","\n","El entrenamiento se detiene cuando el modelo clasifica correctamente todos los ejemplos o llega al máximo de épocas."]},{"cell_type":"code","execution_count":5,"id":"cell-8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-8","executionInfo":{"status":"ok","timestamp":1760482997534,"user_tz":180,"elapsed":11,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"6a41c0e9-0089-47fb-cb5d-1a4ebf6782dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","INICIANDO ENTRENAMIENTO\n","============================================================\n","Tasa de aprendizaje: 0.1\n","Épocas máximas: 50\n","Ejemplos de entrenamiento: 6\n","\n","Época  1: Errores = 1\n","Época  2: Errores = 2\n","Época  3: Errores = 2\n","Época  4: Errores = 1\n","Época  5: Errores = 2\n","Época  6: Errores = 2\n","Época  7: Errores = 0\n","\n","Convergencia alcanzada en época 7\n","\n","============================================================\n","ENTRENAMIENTO FINALIZADO\n","============================================================\n"]}],"source":["# Parámetros de entrenamiento\n","tasa_aprendizaje = 0.1\n","epocas = 50\n","\n","print(\"=\"*60)\n","print(\"INICIANDO ENTRENAMIENTO\")\n","print(\"=\"*60)\n","print(f\"Tasa de aprendizaje: {tasa_aprendizaje}\")\n","print(f\"Épocas máximas: {epocas}\")\n","print(f\"Ejemplos de entrenamiento: {len(X)}\\n\")\n","\n","# Bucle de entrenamiento\n","for epoca in range(epocas):\n","    errores = 0\n","\n","    # Recorremos cada ejemplo\n","    for i in range(len(X)):\n","        x_i = X[i]\n","        y_real = etiquetas[i]\n","        y_pred = predecir(x_i)\n","        error = y_real - y_pred\n","\n","        # Si hay error, ajustamos pesos y bias\n","        if error != 0:\n","            pesos += tasa_aprendizaje * error * x_i\n","            bias += tasa_aprendizaje * error\n","            errores += 1\n","\n","    print(f\"Época {epoca + 1:2d}: Errores = {errores}\")\n","\n","    # Si no hay errores, el modelo convergió\n","    if errores == 0:\n","        print(f\"\\nConvergencia alcanzada en época {epoca + 1}\")\n","        break\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"ENTRENAMIENTO FINALIZADO\")\n","print(\"=\"*60)"]},{"cell_type":"markdown","id":"cell-9","metadata":{"id":"cell-9"},"source":["## 5. Análisis de los pesos aprendidos\n","\n","Después del entrenamiento, los pesos nos indican qué tan importante es cada palabra para determinar el sentimiento.\n","\n","- **Pesos positivos**: Palabras asociadas a sentimiento positivo\n","- **Pesos negativos**: Palabras asociadas a sentimiento negativo\n","- **Pesos cercanos a 0**: Palabras poco informativas"]},{"cell_type":"code","execution_count":6,"id":"cell-10","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-10","executionInfo":{"status":"ok","timestamp":1760483014779,"user_tz":180,"elapsed":8,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"39ba0220-b3c0-4cae-f564-fc31b06ce2a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pesos aprendidos después del entrenamiento:\n","\n","Palabra               Peso Interpretación\n","--------------------------------------------------\n","asado                1.523  → Positivo\n","amo                  0.497  → Positivo\n","detesto              0.367  → Positivo\n","gusta                0.248  → Positivo\n","húmedo               0.143  → Positivo\n","perdí               -0.121  → Negativo\n","bajón               -0.234  → Negativo\n","espectacular        -0.234  → Negativo\n","no                  -0.538  → Negativo\n","calor               -0.869  → Negativo\n","\n","Bias final: 0.100\n","\n","El bias representa el umbral base del modelo.\n"]}],"source":["print(\"Pesos aprendidos después del entrenamiento:\\n\")\n","print(f\"{'Palabra':<15} {'Peso':>10} {'Interpretación'}\")\n","print(\"-\" * 50)\n","\n","for palabra, peso in sorted(zip(vocabulario, pesos), key=lambda x: x[1], reverse=True):\n","    if peso > 0.1:\n","        interpretacion = \"→ Positivo\"\n","    elif peso < -0.1:\n","        interpretacion = \"→ Negativo\"\n","    else:\n","        interpretacion = \"→ Neutral\"\n","\n","    print(f\"{palabra:<15} {peso:>10.3f}  {interpretacion}\")\n","\n","print(f\"\\nBias final: {bias:.3f}\")\n","print(\"\\nEl bias representa el umbral base del modelo.\")"]},{"cell_type":"markdown","id":"cell-11","metadata":{"id":"cell-11"},"source":["## 6. Prueba con nuevas frases\n","\n","Ahora vamos a ver cómo se comporta nuestro perceptrón con frases nuevas que no vio durante el entrenamiento.\n","\n","Esta es la verdadera prueba: ¿puede generalizar lo que aprendió a casos nuevos?"]},{"cell_type":"code","execution_count":7,"id":"cell-12","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-12","executionInfo":{"status":"ok","timestamp":1760483263096,"user_tz":180,"elapsed":16,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"04d9dd50-3d5c-4166-bd31-40334e13c6ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","PREDICCIONES EN FRASES NUEVAS\n","============================================================\n","\n","Frase 1: 'No aguanto este calor'\n","  Vector: [0 1 0 0 0 0 0 0 1 0]\n","  Predicción: Negativo\n","  Palabras clave detectadas: no, calor\n","\n","Frase 2: 'Qué hermoso día para pasear'\n","  Vector: [0 0 0 0 0 0 0 0 0 0]\n","  Predicción: Positivo\n","  No se detectaron palabras del vocabulario\n","\n","Frase 3: 'Detesto levantarme temprano'\n","  Vector: [0 0 0 0 0 0 0 1 0 0]\n","  Predicción: Positivo\n","  Palabras clave detectadas: detesto\n","\n","============================================================\n"]}],"source":["# Frases nuevas para testeo\n","frases_prueba = [\n","    \"No aguanto este calor\",\n","    \"Qué hermoso día para pasear\",\n","    \"Detesto levantarme temprano\"\n","]\n","\n","print(\"=\"*60)\n","print(\"PREDICCIONES EN FRASES NUEVAS\")\n","print(\"=\"*60)\n","\n","# Vectorizamos las frases nuevas\n","X_prueba = np.array([vectorizar(frase, vocabulario) for frase in frases_prueba])\n","predicciones = [predecir(x) for x in X_prueba]\n","\n","# Mostramos los resultados\n","for i, (frase, pred, vector) in enumerate(zip(frases_prueba, predicciones, X_prueba), 1):\n","    resultado = \"Positivo\" if pred == 1 else \"Negativo\"\n","    print(f\"\\nFrase {i}: '{frase}'\")\n","    print(f\"  Vector: {vector}\")\n","    print(f\"  Predicción: {resultado}\")\n","\n","    # Mostramos qué palabras del vocabulario detectó\n","    palabras_detectadas = [vocabulario[j] for j in range(len(vocabulario)) if vector[j] == 1]\n","    if palabras_detectadas:\n","        print(f\"  Palabras clave detectadas: {', '.join(palabras_detectadas)}\")\n","    else:\n","        print(f\"  No se detectaron palabras del vocabulario\")\n","\n","print(\"\\n\" + \"=\"*60)"]},{"cell_type":"markdown","id":"cell-13","metadata":{"id":"cell-13"},"source":["## 7. Reflexión final\n","\n","### ¿Qué aprendimos?\n","\n","1. **Funcionamiento de una neurona artificial básica**: El perceptrón es el bloque fundamental de las redes neuronales. Aprendimos cómo combina entradas ponderadas y aplica una función de activación.\n","\n","2. **Proceso de entrenamiento**: Vimos cómo un modelo aprende ajustando sus pesos iterativamente basándose en los errores que comete. Este principio se extiende a redes neuronales más complejas.\n","\n","3. **Representación de texto**: Usamos bag-of-words, una técnica simple pero efectiva para convertir texto en números que las máquinas pueden procesar.\n","\n","### Limitaciones observadas\n","\n","1. **No considera el orden**: \"No me gusta\" vs \"Me gusta, no\" se representan igual\n","2. **Vocabulario limitado**: Solo conoce las palabras que definimos manualmente\n","3. **Modelo lineal**: Solo puede aprender patrones linealmente separables\n","4. **Sin contexto**: No entiende sarcasmo, ironía o matices del lenguaje\n","5. **Dataset pequeño**: Con solo 6 ejemplos, la generalización es limitada\n","\n","### ¿Qué sigue?\n","\n","En el próximo laboratorio, vamos a ver cómo las **redes neuronales multicapa** (MLP) pueden capturar patrones más complejos usando múltiples perceptrones organizados en capas. Esto nos va a permitir:\n","\n","- Aprender representaciones no lineales\n","- Capturar interacciones entre features\n","- Mejorar la capacidad de generalización\n","\n","Más adelante veremos cómo las **redes recurrentes** (LSTM) pueden procesar el orden de las palabras y, finalmente, cómo los **Transformers** revolucionaron el procesamiento de lenguaje natural."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}